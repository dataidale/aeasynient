{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Authorship attribution and user profiling\n",
    "\n",
    "## Step 1: Get the data\n",
    "\n",
    "Explore the NLTK's interface to the Gutenberg project collection of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you can use `gutenberg.sents` to access individual sentences. \n",
    "\n",
    "Let's select some authors and get some of their works into the training set. Some of the largest number of works in this collection are for *Jane Austen* and *William Shakespeare*, so for the rest of the chapter let's stick with those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]\n",
      "11499\n"
     ]
    }
   ],
   "source": [
    "author1_train = gutenberg.sents('austen-emma.txt') + gutenberg.sents('austen-persuasion.txt')\n",
    "print (author1_train)\n",
    "print (len(author1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']'], ['CHAPTER', '1'], ...]\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "author1_test = gutenberg.sents('austen-sense.txt')\n",
    "print (author1_test)\n",
    "print (len(author1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', 'by', 'William', 'Shakespeare', '1599', ']'], ['Actus', 'Primus', '.'], ...]\n",
      "5269\n"
     ]
    }
   ],
   "source": [
    "author2_train = gutenberg.sents('shakespeare-caesar.txt') + gutenberg.sents(\n",
    "    'shakespeare-hamlet.txt')\n",
    "print (author2_train)\n",
    "print (len(author2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]\n",
      "1907\n"
     ]
    }
   ],
   "source": [
    "author2_test = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "print (author2_test)\n",
    "print (len(author2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, let's check if the two authors produce markedly different texts: estimate the average number of characters per word, number of words per sentence, and *diversity* of author's vocabulary â€“ average number of times each word occurs in a text by the author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n"
     ]
    }
   ],
   "source": [
    "def statistics(gutenberg_data):\n",
    "    for work in gutenberg_data:\n",
    "        num_chars = len(gutenberg.raw(work))\n",
    "        num_words = len(gutenberg.words(work))\n",
    "        num_sents = len(gutenberg.sents(work))\n",
    "        num_vocab = len(set(w.lower() for w in gutenberg.words(work)))\n",
    "        print(round(num_chars/num_words), # average word length in characters\n",
    "             round(num_words/num_sents), # average sentence length in words\n",
    "             round(num_words/num_vocab), # average number of times each word occurs uniquely\n",
    "             work)\n",
    "        \n",
    "gutenberg_data = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt',\n",
    "                 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']\n",
    "statistics(gutenberg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set additional test data\n",
    "\n",
    "To fairly test generalization behavior, let's set additional test data from within the same set of works as we are training the algorithm on. By comparing the algorithm's performance on the set of sentences coming from the same literary works to its performances on a different set of works, you will be able to tell how well the algorithm generalizes above the words it has seen in the training data.\n",
    "\n",
    "First, put the sentences with the author labels together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 16768 sentences\n"
     ]
    }
   ],
   "source": [
    "all_sents = [(sent, \"austen\") for sent in author1_train]\n",
    "all_sents += [(sent, \"shakespeare\") for sent in author2_train]\n",
    "print (f\"Dataset size = {str(len(all_sents))} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, shuffle the data and split it keeping the proportion of the author-speficic data consistent across the training and the same-data testing set. Let's call the test set coming from the same data `pre-test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "values = [author for (sent, author) in all_sents]\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "strat_train_set = []\n",
    "strat_pretest_set = []\n",
    "for train_index, pretest_index in split.split(all_sents, values):\n",
    "    strat_train_set = [all_sents[index] for index in train_index]\n",
    "    strat_pretest_set = [all_sents[index] for index in pretest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the proportions are kept the same across the two data portions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category     Overall   Stratified train  Stratified pretest \n",
      " austen       0.685771  0.685776          0.685748           \n",
      " shakespeare  0.314229  0.314224          0.314252           \n"
     ]
    }
   ],
   "source": [
    "def cat_proportions(data, cat):\n",
    "    count = 0\n",
    "    for item in data:\n",
    "        if item[1]==cat:\n",
    "            count += 1\n",
    "    return float(count) / float(len(data))\n",
    "\n",
    "categories = [\"austen\", \"shakespeare\"]\n",
    "rows = []\n",
    "rows.append([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\"])\n",
    "for cat in categories:\n",
    "    rows.append([cat, f\"{cat_proportions(all_sents, cat):.6f}\", \n",
    "                f\"{cat_proportions(strat_train_set, cat):.6f}\",\n",
    "                f\"{cat_proportions(strat_pretest_set, cat):.6f}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], width=column_widths[i]) \n",
    "                  for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now also initialize the test set in the same way, by adding author labels to the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Category     Overall   Stratified train  Stratified pretest  Test     \n",
      " austen       0.685771  0.685776          0.685748            0.723863 \n",
      " shakespeare  0.314229  0.314224          0.314252            0.276137 \n"
     ]
    }
   ],
   "source": [
    "test_set = [(sent, \"austen\") for sent in author1_test]\n",
    "test_set += [(sent, \"shakespeare\") for sent in author2_test]\n",
    "\n",
    "rows = []\n",
    "rows.append([\"Category\", \"Overall\", \"Stratified train\", \"Stratified pretest\", \"Test\"])\n",
    "for cat in categories:\n",
    "    rows.append([cat, f\"{cat_proportions(all_sents, cat):.6f}\", \n",
    "                f\"{cat_proportions(strat_train_set, cat):.6f}\",\n",
    "                f\"{cat_proportions(strat_pretest_set, cat):.6f}\",\n",
    "                f\"{cat_proportions(test_set, cat):.6f}\"])\n",
    "\n",
    "columns = zip(*rows)\n",
    "column_widths = [max(len(item) for item in col) for col in columns]\n",
    "for row in rows:\n",
    "    print(''.join(' {:{width}} '.format(row[i], width=column_widths[i]) \n",
    "                  for i in range(0, len(row))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run a benchmark model\n",
    "\n",
    "Naive Bayes model from Chapter 2 with words as features can be used as a reasonable approach to set up the benchmark result. Let's first extract the word features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414\n",
      "{'Pol': True, '.': True}\n",
      "{'And': True, 'as': True, 'to': True, 'my': True, 'father': True, ',': True, 'I': True, 'really': True, 'should': True, 'not': True, 'have': True, 'thought': True, 'that': True, 'he': True, 'who': True, 'has': True, 'kept': True, 'himself': True, 'single': True, 'so': True, 'long': True, 'for': True, 'our': True, 'sakes': True, 'need': True, 'be': True, 'suspected': True, 'now': True, '.': True}\n"
     ]
    }
   ],
   "source": [
    "def get_features(text): \n",
    "    features = {}\n",
    "    word_list = [word for word in text]\n",
    "    for word in word_list:\n",
    "        features[word] = True\n",
    "    return features\n",
    "\n",
    "train_features = [(get_features(sents), label) for (sents, label) in strat_train_set]\n",
    "pretest_features = [(get_features(sents), label) for (sents, label) in strat_pretest_set]\n",
    "\n",
    "print(len(train_features))\n",
    "print(train_features[0][0])\n",
    "print(train_features[100][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train NLTK's Naive Bayes classifier on the training data and test it on the pretest data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 13414 sentences\n",
      "Test set size = 3354 sentences\n",
      "Accuracy on the training set = 0.9786789920978083\n",
      "Accuracy on the pretest set = 0.9636255217650567\n",
      "Most Informative Features\n",
      "                    been = True           austen : shakes =    257.7 : 1.0\n",
      "                    King = True           shakes : austen =    197.1 : 1.0\n",
      "                    thou = True           shakes : austen =    191.3 : 1.0\n",
      "                    Lord = True           shakes : austen =     61.2 : 1.0\n",
      "                    doth = True           shakes : austen =     60.4 : 1.0\n",
      "                       d = True           shakes : austen =     58.9 : 1.0\n",
      "                   quite = True           austen : shakes =     55.6 : 1.0\n",
      "                     Tis = True           shakes : austen =     51.6 : 1.0\n",
      "                     She = True           austen : shakes =     43.2 : 1.0\n",
      "                   think = True           austen : shakes =     39.9 : 1.0\n",
      "                    back = True           austen : shakes =     34.4 : 1.0\n",
      "                     has = True           austen : shakes =     34.2 : 1.0\n",
      "                  father = True           austen : shakes =     32.3 : 1.0\n",
      "                  coming = True           austen : shakes =     29.5 : 1.0\n",
      "                  moment = True           austen : shakes =     29.1 : 1.0\n",
      "                 looking = True           austen : shakes =     28.6 : 1.0\n",
      "                       l = True           shakes : austen =     28.4 : 1.0\n",
      "                    mind = True           austen : shakes =     28.3 : 1.0\n",
      "                     far = True           austen : shakes =     26.1 : 1.0\n",
      "                   years = True           austen : shakes =     25.8 : 1.0\n",
      "                   known = True           austen : shakes =     25.5 : 1.0\n",
      "                  mother = True           austen : shakes =     25.5 : 1.0\n",
      "                     Nor = True           shakes : austen =     25.5 : 1.0\n",
      "                  hardly = True           austen : shakes =     24.9 : 1.0\n",
      "                carriage = True           austen : shakes =     24.9 : 1.0\n",
      "                   party = True           austen : shakes =     24.5 : 1.0\n",
      "                     ere = True           shakes : austen =     24.0 : 1.0\n",
      "                     few = True           austen : shakes =     23.7 : 1.0\n",
      "                 account = True           austen : shakes =     23.7 : 1.0\n",
      "                    poor = True           austen : shakes =     23.0 : 1.0\n",
      "                 feeling = True           austen : shakes =     22.8 : 1.0\n",
      "                     she = True           austen : shakes =     22.7 : 1.0\n",
      "                   among = True           austen : shakes =     22.1 : 1.0\n",
      "                 brother = True           austen : shakes =     21.8 : 1.0\n",
      "                  assure = True           austen : shakes =     21.2 : 1.0\n",
      "                 Brother = True           shakes : austen =     21.1 : 1.0\n",
      "                    seen = True           austen : shakes =     20.6 : 1.0\n",
      "              afterwards = True           austen : shakes =     19.7 : 1.0\n",
      "                 manners = True           austen : shakes =     19.7 : 1.0\n",
      "                    Mark = True           shakes : austen =     19.6 : 1.0\n",
      "                 whether = True           austen : shakes =     19.1 : 1.0\n",
      "                    care = True           austen : shakes =     18.5 : 1.0\n",
      "                    mean = True           austen : shakes =     18.5 : 1.0\n",
      "                       3 = True           shakes : austen =     18.2 : 1.0\n",
      "                       4 = True           shakes : austen =     18.2 : 1.0\n",
      "                 Letters = True           shakes : austen =     18.2 : 1.0\n",
      "               beginning = True           austen : shakes =     17.6 : 1.0\n",
      "                 husband = True           austen : shakes =     17.6 : 1.0\n",
      "                 company = True           austen : shakes =     17.3 : 1.0\n",
      "                 imagine = True           austen : shakes =     17.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier, classify\n",
    "\n",
    "print (f\"Training set size = {str(len(train_features))} sentences\")\n",
    "print (f\"Pretest set size = {str(len(pretest_features))} sentences\")\n",
    "# train the classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "print (f\"Accuracy on the training set = {str(classify.accuracy(classifier, train_features))}\")\n",
    "print (f\"Accuracy on the pretest set = {str(classify.accuracy(classifier, pretest_features))}\")    \n",
    "# check which words are most informative for the classifier\n",
    "classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this performance to the performance on the new test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size = 6906 sentences\n",
      "Accuracy on the test set = 0.895742832319722\n"
     ]
    }
   ],
   "source": [
    "test_features = [(get_features(sents), label) for (sents, label) in test_set]\n",
    "print (f\"Test set size = {str(len(test_features))} sentences\")\n",
    "print (f\"Accuracy on the test set = {str(classify.accuracy(classifier, test_features))}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run a different model with a different set of features \n",
    "\n",
    "**Feature types 1 and 2 â€“ simple statistics over the length of words**: Take average number of words per sentence and average number of characters per words as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5714285714285716\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "#average word length in characters\n",
    "def avg_number_chars(text):\n",
    "    total_chars = 0.0\n",
    "    for word in text:\n",
    "        total_chars += len(word)\n",
    "    return float(total_chars)/float(len(text))\n",
    "\n",
    "#length in terms of words\n",
    "def number_words(text):\n",
    "    return float(len(text))\n",
    "\n",
    "print(avg_number_chars([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))\n",
    "print(number_words([\"Not\", \"so\", \"happy\", \",\", \"yet\", \"much\", \"happyer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent all data sets with their feature sets. You will need to initialize a feature set for each text and map it to the author. In addition, let's switch to numerical representation of author labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n"
     ]
    }
   ],
   "source": [
    "def initialize_dataset(source):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "train_data, train_targets = initialize_dataset(strat_train_set)\n",
    "pretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\n",
    "test_data, test_targets = initialize_dataset(test_set)\n",
    "\n",
    "print (len(train_data), len(train_targets))\n",
    "print (len(pretest_data), len(pretest_targets))\n",
    "print (len(test_data), len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply classification with the `sklearn`'s `Decision Trees` classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "text_clf = DecisionTreeClassifier(random_state=42)\n",
    "text_clf.fit(train_data, train_targets)  \n",
    "predicted = text_clf.predict(pretest_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation including the following metrics: *accuracy*, *confusion matrix*, *precision*, *recall* and *F1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7972570065593322\n",
      "[[2132  168]\n",
      " [ 512  542]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.93      0.86      2300\n",
      "          1       0.76      0.51      0.61      1054\n",
      "\n",
      "avg / total       0.79      0.80      0.78      3354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(predicted, targets):\n",
    "    print(np.mean(predicted == targets))\n",
    "    print(metrics.confusion_matrix(targets, predicted))\n",
    "    print(metrics.classification_report(targets, predicted))\n",
    "    \n",
    "evaluate(predicted, pretest_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049522154648132\n",
      "[[4605  394]\n",
      " [ 953  954]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.92      0.87      4999\n",
      "          1       0.71      0.50      0.59      1907\n",
      "\n",
      "avg / total       0.80      0.80      0.79      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(test_data)\n",
    "evaluate(predicted, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a much more generalizable set of features â€“ the performance on both set is very close ($0.7973$ vs. $0.8050$). Besides, it contains only $2$ features as opposed to over $13K$ words! However, now the performance is much lower than with words. Let's try and improve it further.\n",
    "\n",
    "**Feature type 3 â€“ count of stopwords**: Add `spaCy` functionality and see how only a handful of frequent words are distributed in texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a very general method that can be applied to any type of words\n",
    "def word_counts(text):\n",
    "    counts = {}\n",
    "    for word in text:\n",
    "        counts[word.lower()] = counts.get(word.lower(), 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's augment our feature set with the counts of stopwords only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n"
     ]
    }
   ],
   "source": [
    "def initialize_dataset(source):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        counts = word_counts(sent)\n",
    "        for word in STOP_WORDS:\n",
    "            if word in counts.keys():\n",
    "                feature_list.append(counts.get(word))\n",
    "            else:\n",
    "                feature_list.append(0)        \n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "train_data, train_targets = initialize_dataset(strat_train_set)\n",
    "pretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\n",
    "test_data, test_targets = initialize_dataset(test_set)\n",
    "\n",
    "print (len(train_data), len(train_targets))\n",
    "print (len(pretest_data), len(pretest_targets))\n",
    "print (len(test_data), len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and test on both pretest and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8091830649970185\n",
      "[[1964  336]\n",
      " [ 304  750]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.85      0.86      2300\n",
      "          1       0.69      0.71      0.70      1054\n",
      "\n",
      "avg / total       0.81      0.81      0.81      3354\n",
      "\n",
      "0.8068346365479293\n",
      "[[4220  779]\n",
      " [ 555 1352]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.84      0.86      4999\n",
      "          1       0.63      0.71      0.67      1907\n",
      "\n",
      "avg / total       0.81      0.81      0.81      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = DecisionTreeClassifier(random_state=42)\n",
    "text_clf.fit(train_data, train_targets)  \n",
    "predicted = text_clf.predict(pretest_data)\n",
    "evaluate(predicted, pretest_targets)\n",
    "\n",
    "predicted = text_clf.predict(test_data)\n",
    "evaluate(predicted, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight improvement in accuracy of about $0.005$ (or $0.5\\%$), up to $0.8146$-$0.8142$ and $0.8096$-$0.8093$. However, what is most interesting about this feature is that there is now a more considerable improvement in performance metrics on the minority class (*shakespeare*): on the pretest set recall rises from $0.51$ to $0.72$ and F1 from $0.61$ to $0.71$ â€“ a whole of $10$ points; on the test set the improvement in recall is from $0.50$ to $0.71$ and in F1 from $0.59$ to $0.67$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature type 4 â€“ proportion of stopwords**: Estimate what proportion of words in sentence are stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n"
     ]
    }
   ],
   "source": [
    "def proportion_words(text, wordlist):\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word.lower() in wordlist:\n",
    "            count += 1\n",
    "    return float(count)/float(len(text))\n",
    "\n",
    "def initialize_dataset(source):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        counts = word_counts(sent)\n",
    "        for word in STOP_WORDS:\n",
    "            if word in counts.keys():\n",
    "                feature_list.append(counts.get(word))\n",
    "            else:\n",
    "                feature_list.append(0)        \n",
    "        feature_list.append(proportion_words(sent, STOP_WORDS))\n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "train_data, train_targets = initialize_dataset(strat_train_set)\n",
    "pretest_data, pretest_targets = initialize_dataset(strat_pretest_set)\n",
    "test_data, test_targets = initialize_dataset(test_set)\n",
    "\n",
    "print (len(train_data), len(train_targets))\n",
    "print (len(pretest_data), len(pretest_targets))\n",
    "print (len(test_data), len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, train and test on both pretest and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8145497912939773\n",
      "[[1990  310]\n",
      " [ 312  742]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.87      0.86      2300\n",
      "          1       0.71      0.70      0.70      1054\n",
      "\n",
      "avg / total       0.81      0.81      0.81      3354\n",
      "\n",
      "0.8146539241239502\n",
      "[[4285  714]\n",
      " [ 566 1341]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.86      0.87      4999\n",
      "          1       0.65      0.70      0.68      1907\n",
      "\n",
      "avg / total       0.82      0.81      0.82      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = DecisionTreeClassifier(random_state=42)\n",
    "text_clf.fit(train_data, train_targets)  \n",
    "predicted = text_clf.predict(pretest_data)\n",
    "evaluate(predicted, pretest_targets)\n",
    "\n",
    "predicted = text_clf.predict(test_data)\n",
    "evaluate(predicted, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an even further small improvement: $0.8154$ and $0.8154$-$0.8180$. Moreover, performance on both pretest and test sets is very similar now. However, overall perfomance is still not as good as what you've got with words, so let's keep going.\n",
    "\n",
    "**Feature type 5 â€“ proportion of words of specific parts of speech**: Just like you added proportion of stopwords, add proportions of words of specific parts of speech. For that, first, preprocess the sentences and for each of them keep a dictionary mapping each sentence to its `spaCy`'s representation with all language-related fields (this might take some time due to processing run by `spaCy`, so let's add some code to track how many sentences have been processed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 texts processed\n",
      "4000 texts processed\n",
      "6000 texts processed\n",
      "8000 texts processed\n",
      "10000 texts processed\n",
      "12000 texts processed\n",
      "Dataset processed\n",
      "2000 texts processed\n",
      "Dataset processed\n",
      "2000 texts processed\n",
      "4000 texts processed\n",
      "6000 texts processed\n",
      "Dataset processed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(source):\n",
    "    source_docs = {}\n",
    "    index = 0\n",
    "    for (sent, label) in source:\n",
    "        text = \" \".join(sent)\n",
    "        source_docs[text] = nlp(text)\n",
    "        if index>0 and (index%2000)==0:\n",
    "            print(str(index) + \" texts processed\")\n",
    "        index += 1\n",
    "    print(\"Dataset processed\")\n",
    "    return source_docs\n",
    "\n",
    "train_docs = preprocess(strat_train_set)\n",
    "pretest_docs = preprocess(strat_pretest_set)\n",
    "test_docs = preprocess(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the PoS counts as a feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "pos_list = [\"C\", \"D\", \"E\", \"F\", \"I\", \"J\", \"M\", \"N\", \"P\", \"R\", \"T\", \"U\", \"V\", \"W\"]\n",
    "\n",
    "def pos_counts(text, source_docs, pos_list):\n",
    "    pos_counts = {}\n",
    "    doc = source_docs.get(\" \".join(text))\n",
    "    tags = []\n",
    "    for word in doc:\n",
    "        tags.append(str(word.tag_)[0])\n",
    "    counts = Counter(tags)\n",
    "    for pos in pos_list:\n",
    "        if pos in counts.keys():\n",
    "            pos_counts[pos] = counts.get(pos)\n",
    "        else: pos_counts[pos] = 0\n",
    "    return pos_counts\n",
    "\n",
    "def initialize_dataset(source, source_docs):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        counts = word_counts(sent)\n",
    "        for word in STOP_WORDS:\n",
    "            if word in counts.keys():\n",
    "                feature_list.append(counts.get(word))\n",
    "            else:\n",
    "                feature_list.append(0)        \n",
    "        feature_list.append(proportion_words(sent, STOP_WORDS))\n",
    "        p_counts = pos_counts(sent, source_docs, pos_list)\n",
    "        for pos in p_counts.keys():\n",
    "            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "train_data, train_targets = initialize_dataset(strat_train_set, train_docs)\n",
    "pretest_data, pretest_targets = initialize_dataset(strat_pretest_set, pretest_docs)\n",
    "test_data, test_targets = initialize_dataset(test_set, test_docs)\n",
    "\n",
    "print (len(train_data), len(train_targets))\n",
    "print (len(pretest_data), len(pretest_targets))\n",
    "print (len(test_data), len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as before, train, test and evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8240906380441264\n",
      "[[1997  303]\n",
      " [ 287  767]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.87      0.87      2300\n",
      "          1       0.72      0.73      0.72      1054\n",
      "\n",
      "avg / total       0.82      0.82      0.82      3354\n",
      "\n",
      "0.8285548798146539\n",
      "[[4369  630]\n",
      " [ 554 1353]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.87      0.88      4999\n",
      "          1       0.68      0.71      0.70      1907\n",
      "\n",
      "avg / total       0.83      0.83      0.83      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = DecisionTreeClassifier(random_state=42)\n",
    "text_clf.fit(train_data, train_targets)  \n",
    "predicted = text_clf.predict(pretest_data)\n",
    "evaluate(predicted, pretest_targets)\n",
    "\n",
    "predicted = text_clf.predict(test_data)\n",
    "evaluate(predicted, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement with this feature reaches $0.8256$-$0.8226$ and $0.8310$-$0.8251$.\n",
    "\n",
    "For convenience, let's pack up the datasets initialization, training, testing and evaluation into a method, since we don't change any code in this bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n",
      "\n",
      "0.8240906380441264\n",
      "[[1997  303]\n",
      " [ 287  767]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.87      0.87      2300\n",
      "          1       0.72      0.73      0.72      1054\n",
      "\n",
      "avg / total       0.82      0.82      0.82      3354\n",
      "\n",
      "0.8285548798146539\n",
      "[[4369  630]\n",
      " [ 554 1353]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.87      0.88      4999\n",
      "          1       0.68      0.71      0.70      1907\n",
      "\n",
      "avg / total       0.83      0.83      0.83      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    train_data, train_targets = initialize_dataset(strat_train_set, train_docs)\n",
    "    pretest_data, pretest_targets = initialize_dataset(strat_pretest_set, pretest_docs)\n",
    "    test_data, test_targets = initialize_dataset(test_set, test_docs)\n",
    "\n",
    "    print (len(train_data), len(train_targets))\n",
    "    print (len(pretest_data), len(pretest_targets))\n",
    "    print (len(test_data), len(test_targets))\n",
    "    print ()\n",
    "    \n",
    "    text_clf = DecisionTreeClassifier(random_state=42)\n",
    "    text_clf.fit(train_data, train_targets)  \n",
    "    predicted = text_clf.predict(pretest_data)\n",
    "    evaluate(predicted, pretest_targets)\n",
    "\n",
    "    predicted = text_clf.predict(test_data)\n",
    "    evaluate(predicted, test_targets)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement with this feature reaches $0.8256$-$0.8226$ and $0.8310$-$0.8251$.\n",
    "\n",
    "Let's add further linguistic feature â€“ for instance, suffixes that are already stored in the `docs`.\n",
    "\n",
    "**Feature type 6 â€“ count selected suffixes**: As the number of suffixes will be quite large (smaller than the number of words, though), let's set a cutoff point to, e.g. the top $40\\%$ of the suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "[',', '.', 'the', 'and', 'to', 'ing', 'of', 'her', '\"', 'a', 'i', 'hat', 'it', 'in', ';', 'was', 'not', 'she', 'you', 'his', 'uld', 'be', 'he', 'ere', 'had', \"'\", 'as', '--', 'all', 'ion', 'but', 'for', 'ith', 'ery', 'is', 'ave', 'ent', 'ill', 'nce', 'ght', 'ter', 'at', 'my', 'our', 'so', 'him', 'een', '?', 's', 'uch', 'ore', 'ome', 'mr', 'ver', 'are', 'ted', 'one', ':', 'ble', 'ell', 'no', 'on', 'now', 'any', 'ust', 'by', 'me', '!', 'hen', 'hey', 'out', 'ess', 'ich', '-', 'do', 'ure', 'mrs', 'ain', 'rom', 'elf', 'red', 'or', 'ood', 'if', 'mma', 'use', 'aid', 'hem', 'ton', 'ely', 'sed', 'own', 'est', 'man', 'an', 'ost', 'ake', 'ers', 'ear', 'nly', '_', 'we', 'iss', 'ned', 'ugh', '.--', 'lly', 'eir', 'ied', 'end', 'am', 'han', 'ons', 'did', 'ame', 'can', 'tle', 'ite', 'ose', 'ate', 'tly', 'how', 'ant', 'ard', 'ast', 'ong', 'ive', 'ity', 'who', 'ine', 'ved', 'eat', 'ect', 'iet', 'nne', 'ded', 'ink', 'way', 'ord', 'ous', 'ays', 'ime', 'ked', 'ady', 'say', 'age', 'ike', 'old', 'ies', 'eed', 'ngs', 'too', 'd', 'ind', 'les', 'ade', 'der', 'aue', 'see', 'med', 'ort', 'ile', 'day', 'rst', 'ley', 'rth', 'its', 'ace', 'may', 'art', 'ove', 'ody', 'let', 'ice', 'nds', 'ane', 'ful', 'led', 'ish', 'oon', 'has', 'und', 'yes', 'nts', 'ven', 'tes', 'ath', 'pon', 'rds', 'rry', 'hed', 'ged', 'ary', 'ead', 'son', 'ple', 'nto', 'two', 'mes', 'ily', 'go', 'ife', 'ten', 'ung', 'ook', 'ise', 'ner', 'ius', 'ral', 'sir', 'iot', 'dly', 'rly', 'rse', 'ank', 'oor', 'ger', 'rty', 'up', '(', 'ide', ')', 'men', 'ree', 'low', 'ves', 'ced', 'new', 'oth', 'yet', 'fax', 'res', 'wer', 'eal', 'ses', 'oes', 'sse', 'ars', 'ase', 'ces', 'eld', 'ual', 'ppy', 'ope', 'oom', 'ole', 'ond', 'elt', 'hou', 'ken', 'per', 'off', 'bly', 'alk', 'ese', 'lay', 'ick', 'us', 'sit', 'unt', 'thy', 'tus', 'oke', 'why', 'iue', 'ire', 'nor', 'alf', 'saw', 'sar', 'ack', 'nge', 'kes', 'wed', 'int', 'oh', 'eet', 'hee', 'ach', 'get', 'eak', 'ber', 'dge', 'few', 'lls', 'row', 'air', 'aps', 'lfe', 'urs', 'rld', 'wne', 'nes', 'uth', 'tch', 'ull', 'eve', 'rue', 'arm', 'o', 'ury', 'eel', 'cke', 'lse', 'urn', 'ene', 'set', '`', 'ety', 'dea', 'oue', 'ubt', 'nse', 'try', 'eft', 'put', 'ues', 'uer', 'vs', 'ins', 'hts', 'ude', 'ped', 'eth', 'tty', 'ony', 'isa', 't', 'nst', 'oss', 'mer', 'ept', 'ued', 'ans', 'hom', 'uen', 'ext', 'act', 'far', 'don', 'lar', 'rge', 'sly', 'nke', 'hes', 'ors', 'ret', 'ren', 'bad', 'nch', 'ohn', 'gly', 'lad', 'une', 'rit', 'ory', 'igh', 'tis', 'ior', 'hin', 'pen', 'eep', 'rve', 'nty', 'ild', 'gan', 'wes', 'tay', 'hew', 'tin', 'rke', 'ial', 'ges', 'oft', 'fer', 'rne', 'yed', 'des', 'ote', 'nal', 'ray', 'uty', 'ert', 'ste', 'nde', 'den', 'ets', 'ean', 'ows', 'ncy', 'cle', 'hus', 'hip', 'cts', 'ist', 'lla', 'tta', 'mon', 'got', 'hly', 'ews', 'eye', 'th', 'ize', 'ule', 'vp', 'lth', 'epe', 'ete', 'met', 'ult', 'yme', 'rts', 'lle', 'pes', 'st', 'eem', 'uck', 'fit', 'ute', 'ply', 'nel', 'vil', 'doe', 'eek', 'ier', 'elp', 'gin', 'oks', 'lor', 'ths', 'tie', 'bed', 'asy', 'ens', 'ech', 'val', 'gue', 'ass', 'ale', 'dow', 'acy', 'eme', 'zed', 'rew', 'ark', 'ape', 'irl', 'rer', 'ool', 'uit', 'mpt', 'lty', 'yle', 'irs', 'ece', 'rms', 'uct', 'xed', 'ern', 'iew', 'ago', 'ney', 'oad', 'afe', 'ask', 'rme', 'rch', 'rre', 'ork', 'ief', 'oms', 'xon', 'nay', 'rie', 'cal', 'nks', 'boy', 'joy', 'eks', 'ods', 'sts', 'god', 'l', 'mbe', 'sat', 'als', 'uce', 'bey', 'lue', 'ler', 'lan', 'apa', 'ush', 'lis', 'orm', 'ock', 'sea', 'gle', '&', 'dle', 'ait', 'gth', 're', 'due', 'oin', 'oof', 'sin', 'ims', 'top', 'ems', 'law', 'die', 'uly', 'tio', 'tal', 'eas', 'sad', 'mit', 'ian', 'mad', 'lso', 'fts', 'nth', 'oud', 'run', 'wee', 'gun', 'lts', 'usy', 'sic', 'gry', 'iam', 'ees', 'aes', 'els', 'iod', 'ror', 'esh', 'ume', 'rce', 'tor', 'bid', 'lye', 'eds', 'awn', 'gge', 'ker', 'lke', 'raw', 'ha', 'oot', 'lds', 'ser', 'odd', 'ska', 'rls', 'aim', 'dit', 'mly', 'pay', 'sty', 'erd', 'ilt', 'ede', 'dom', 'ils', 'tic', 'tea', 'lia', 'box', 'bit', 'net', 'cut', 'oat', 'sal', 'ash']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def select_suffixes(cutoff):\n",
    "    all_suffixes = []\n",
    "    for doc in train_docs.values():\n",
    "        for word in doc:\n",
    "            all_suffixes.append(str(word.suffix_).lower())\n",
    "    counts = Counter(all_suffixes)\n",
    "    sorted_counts = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    selected_suffixes = []\n",
    "    for i in range(0, round(len(counts)*cutoff)):\n",
    "        selected_suffixes.append(sorted_counts[i][0])\n",
    "    return selected_suffixes\n",
    "    \n",
    "selected_suffixes = select_suffixes(0.4)\n",
    "print(len(selected_suffixes))\n",
    "print(selected_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def suffix_counts(text, source_docs, suffix_list):\n",
    "    suffix_counts = {}\n",
    "    doc = source_docs.get(\" \".join(text))\n",
    "    suffixes = []\n",
    "    for word in doc:\n",
    "        suffixes.append(str(word.suffix_))\n",
    "    counts = Counter(suffixes)\n",
    "    for suffix in suffix_list:\n",
    "        if suffix in counts.keys():\n",
    "            suffix_counts[suffix] = counts.get(suffix)\n",
    "        else: suffix_counts[suffix] = 0\n",
    "    return suffix_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n",
      "\n",
      "0.9558735837805605\n",
      "[[2224   76]\n",
      " [  72  982]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2300\n",
      "          1       0.93      0.93      0.93      1054\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3354\n",
      "\n",
      "0.9514914567043151\n",
      "[[4857  142]\n",
      " [ 193 1714]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97      4999\n",
      "          1       0.92      0.90      0.91      1907\n",
      "\n",
      "avg / total       0.95      0.95      0.95      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_dataset(source, source_docs):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        counts = word_counts(sent)\n",
    "        for word in STOP_WORDS:\n",
    "            if word in counts.keys():\n",
    "                feature_list.append(counts.get(word))\n",
    "            else:\n",
    "                feature_list.append(0)        \n",
    "        feature_list.append(proportion_words(sent, STOP_WORDS))\n",
    "        p_counts = pos_counts(sent, source_docs, pos_list)\n",
    "        for pos in p_counts.keys():\n",
    "            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n",
    "        s_counts = suffix_counts(sent, source_docs, selected_suffixes)\n",
    "        for suffix in s_counts.keys():\n",
    "            feature_list.append(float(s_counts.get(suffix))/float(len(sent)))\n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This feature brings the largest improvement: up to $0.9568$-$0.9535$ and $0.9535$-$0.9524$.\n",
    "\n",
    "Finally, let's collect specific (non-overlapping) vocabularies per each author.\n",
    "\n",
    "**Feature type 7 â€“ count words that are specific for each author**: First collect the set of words that is unique for each author (i.e., the words that only Shakespeare or only Austen uses) and then count their occurrences across the data sets. You can introduce a cutoff as before and only use, e.g., top $50\\%$ of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4435\n",
      "['\"', 'have', '--', '.\"', 'mr', 'mrs', 'emma', 'miss', 'than', '.--', ',\"', 'only', 'every', 'never', 'harriet', 'anne', 'herself', 'own', 'weston', 'knightley', 'elton', 'again', 'always', 'soon', '!--', '?\"', 'captain', 'jane', 'woodhouse', 'dear', 'elliot', 'ever', 'up', 'just', 'having', 'give', 'himself', 'fairfax', 'over', 'upon', 'seemed', 'wentworth', '!\"', 'churchill', 'however', '?--', 'even', 'felt', 'really', 'frank', 'us', 'its', 'room', 'half', 'feelings', 'hartfield', 'certainly', 'charles', 'smith', 'bates', 'russell', 'believe', 'love', 'family', 'evening', 'feel', 'walter', ';--', 'hear', 'looked', 'idea', 'deal', 'acquaintance', 'myself', 'highbury', 'down', 'between', 'musgrove', 'mary', 'hour', 'subject', '`', 'louisa', 'perfectly', 'suppose', 'under', 'general', 'obliged', 'happiness', 'able', 'wanted', 'replied', 'given', 'john', 'talked', 'passed', 'elizabeth', '--\"', 'understand', 'nobody', 'leave', 'kind', 'less', 'interest', 'near', 'attention', 'situation', 'randalls', 'martin', 'agreeable', 'walked', 'perry', 'chapter', 'afraid', 'equal', 'extremely', 'gave', 'themselves', 'attachment', 'natural', 'kellynch', 'business', 'henrietta', 'wished', 'usual', 'talking', 'door', 'called', 'minutes', 'conversation', 'used', 'benwick', 'days', 'child', 'degree', 'appeared', 'object', 'uppercross', 'isabella', 'use', 'particularly', 'pleased', 'received', 'harville', 'superior', 'means', 'different', 'lyme', 'admiral', 'goddard', 'took', 'colonel', 'son', 'help', 'returned', 'yourself', 'forward', 'hoped', 'asked', 'giving', 'air', 'cole', 'real', 'added', 'society', 'handsome', 'croft', 'expected', 'believed', 'continued', 'year', 'settled', 'kindness', 'girl', 'four', 'weather', 'anything', 'supposed', 'five', 'appearance', 'entirely', 'engaged', 'anxious', 'fond', 'probably', 'turned', 'campbell', 'engagement', 'london', 'lived', 'hours', 'circumstances', 'taylor', 'delighted', 'pain', 'advantage', 'week', 'warm', 'curiosity', 'delightful', 'donwell', 'decided', 'comfortable', 'evil', 'dixon', 'loved', 'everything', 'understanding', 'style', 'persuaded', 'conduct', 'sat', 'amiable', 'easy', 'generally', 'news', 'looks', 'completely', 'self', 'understood', 'fair', 'tone', 'knowing', 'dancing', 'mentioned', \",'\", 'obliging', 'plan', 'regret', 'favour', 'living', 'serious', ',--', 'wanting', 'fact', 'aware', 'agreed', 'convinced', 'especially', 'disposed', ',)', 'pass', \".'\", 'knows', 'influence', 'happened', 'elegant', 'hayter', 'actually', 'papa', 'wallis', 'gratitude', 'extraordinary', 'observed', 'interesting', 'marrying', 'says', 'justice', 'pity', 'merely', 'absolutely', 'cheerful', 'observe', 'attentions', 'distress', 'clever', 'trying', 'compliment', 'rooms', 'abbey', 'judge', 'charming', 'instead', 'spite', 'enscombe', 'join', 'proof', 'waiting', 'summer', 'whenever', 'calling', 'difficulty', 'seems', 'recollect', 'vain', 'certain', '),', 'live', 'staying', 'useful', 'hint', 'claims', 'greatest', 'event', 'invitation', 'weeks', 'service', 'complete', 'musgroves', 'period', 'enjoyment', 'attached', 'tired', 'grove', 'fully', ';\"', 'surprize', 'connexion', 'telling', 'entered', 'distance', 'parties', 'music', 'expect', 'consciousness', 'occupied', 'lately', 'cousin', 'itself', 'mere', 'listened', 'odd', 'beautiful', 'maple', 'judgment', 'mistaken', 'hers', '.--\"', 'fixed', 'anxiety', 'cottage', 'resolved', 'written', 'shepherd', 'recommend', 'guess', 'ourselves', 'scarcely', 'equally', 'occurred', 'admitted', 'considering', 'miles', 'liked', 'agitation', 'information', 'consideration', 'wishing', 'desirable', 'difficulties', 'robert', 'plain', 'form', 'tea', 'hurry', 'imagined', 'box', 'remained', 'nearly', 'spent', 'thinks', 'ball', 'girls', 'glance', 'whatever', 'circle', 'rain', 'reached', 'getting', 'dalrymple', 'comprehend', 'spend', 'fortnight', ':--', 'opportunity', 'approbation', 'around', 'move', 'education', 'vanity', 'deserve', 'joined', 'uncle', 'confidence', 'compliments', 'henry', 'favourite', 'breakfast', 'moments', 'dreadful', 'reasonable', 'scheme', 'rational', 'future', 'terms', 'proved', 'opened', 'behaviour', 'neighbourhood', 'joy', 'camden', 'crofts', 'thrown', '_she_', 'severe', 'concern', 'warmth', 'promised', 'smiled', 'returning', 'escape', 'intercourse', 'frederick', 'admired', 'eltons', 'dance', 'confusion', 'ma', 'suspicion', 'busy', 'sick', 'intimacy', 'surprise', 'loss', 'indifference', 'produced', 'chuse', 'amusement', 'encouragement', 'description', 'suspect', 'neighbours', 'charade', 'ideas', 'excepting', 'drew', 'belong', 'totally', 'stopped', 'repeated', 'suit', 'seriously', 'evidently', 'following', 'closed', 'conviction', 'warmly', 'arrived', 'acknowledged', 'observation', 'campbells', 'infinitely', 'differently', 'disappointment', 'inferior', 'elegance', 'sudden', 'exclaimed', 'winter', 'anywhere', 'proposed', 'suffered', 'views', 'boys', 'james', 'somebody', 'respectable', 'intimate', 'required', 'grateful', 'forced', 'seven', 'road', 'disagreeable', 'amused', 'regular', 'prevent', 'clear', 'ways', 'laughing', 'sensations', 'public', 'attending', 'introduced', 'interrupted', 'attend', 'shewed', 'lively', 'receiving', 'deep', 'prepared', 'invited', 'hints', 'explanation', 'chair', 'nonsense', 'servants', 'necessity', 'misery', 'persuade', 'declare', 'avoid', 'decidedly', 'charm', 'comparison', 'hearted', 'alarm', 'dearest', 'possibly', 'favourable', 'properly', 'personal', 'cousins', 'unhappy', 'enjoy', 'considerable', 'gratified', 'feared', 'recovered', 'during', 'leaving', 'book', 'naturally', 'apparent', 'habit', 'arranged', 'likeness', 'improved', 'nurse', 'drawn', 'thoroughly', 'happier', 'six', 'visits', 'dress', 'conscious', 'assistance', 'luck', 'refused', 'solicitude', 'writing', 'christmas', 'prospect', 'settle', 'earnestly', 'motive', 'subjects', 'arrival', 'concerned', 'objection', 'concert', '_her_', 'increased', 'daily', 'evident', 'fancied', 'venture', 'tolerably', 'delicacy', 'material', 'claim', 'assist', 'indulgence', 'continually', 'acknowledge', 'excessively', 'compassion', 'several', 'gradually', 'importance', 'intelligible', 'undoubtedly', 'eat', 'journey', 'judged', 'confess', 'features', 'contrary', 'assured', 'dislike', 'deserved', 'arrangement', 'recollection', 'astonished', 'private', 'intelligence', 'wedding', 'everybody', 'books', 'pianoforte', 'attentive', 'communication', 'eagerness', 'astonishment', 'occur', 'express', 'unpleasant', 'receive', '\"--', 'direction', '.,', 'folly', 'employment', 'particulars', 'separate', 'utmost', 'expression', 'above', 'servant', 'induced', 'questions', 'vicarage', 'lucky', 'exquisite', 'drive', 'laughed', 'mortification', 'coles', 'civility', 'relief', 'grown', 'steady', 'judging', 'advice', 'kindly', 'invite', 'persuasion', 'prove', 'superiority', 'appears', 'possibility', 'families', 'suspense', 'interested', 'lovely', 'fancying', '_you_', 'preferred', 'entering', 'accept', 'ford', 'endeavour', 'doors', 'niece', 'played', 'autumn', 'civil', ',\"--', '--(', ')--', 'hawkins', 'pretend', 'apples', 'human', 'churchills', 'weymouth', 'shewn', 'connexions', 'admire', 'anybody', 'connected', 'harm', 'domestic', 'dining', 'distinction', 'quitted', 'felicity', 'painful', 'daughters', 'gallantry', 'interference', 'mile', 'buildings', 'cared', 'listening', 'income', 'sufficient', 'united', 'remarkably', 'becoming', 'harvilles', 'gives', 'altered', 'habits', 'observing', 'navy', 'expressed', 'dark', 'wonderful', 'visited', 'affected', 'sentiments', 'illness', 'inn', 'seldom', 'scruple', 'notions', 'frightened', 'readily', 'musical', 'heir', 'support', 'satisfy', 'miserable', 'suspected', 'larkins', 'probable', 'authority', 'soul', 'removal', 'capable', 'pains', 'convince', 'supposing', 'intention', 'silly', 'introduction', 'lives', 'perceive', 'doubtful', 'keeping', 'interval', 'advantages', 'divided', 'rendered', 'uneasy', 'certainty', 'employed', 'valuable', 'nerves', 'affairs', 'success', 'conclusion', 'sentiment', 'principal', 'horror', 'melancholy', 'grandmama', 'tells', 'widow', 'secured', 'dared', 'declared', 'parish', 'tolerable', 'expecting', 'ceased', 'independence', 'add', 'card', 'convey', 'sensation', 'tenderness', 'bye', 'comforts', 'perfection', 'disengaged', 'conveyed', 'consequently', 'bless', 'cobb', 'confined', 'earlier', 'quarrel', 'afford', 'communicated', 'addressing', 'stir', 'square', 'desired', 'encouraging', 'suckling', '_that_', 'pleasantly', 'evils', 'cheeks', 'martins', 'moved', 'pork', 'housekeeper', 'sought', 'november', 'proposal', 'agitated', 'expressions', 'removed', 'ventured', 'carteret', 'unless', 'inquiries', 'lodgings', 'shewing', 'emotion', 'suspicions', 'begged', 'sunk', 'blessed', 'politeness', 'reproach', 'composure', 'delicate', 'village', 'reserve', 'hesitation', 'bustle', 'approach', 'watching', 'grieved', 'pretence', 'presume', 'accepted', 'mistress', 'increase', 'moving', 'farm', 'parlour', 'fairly', 'hair', 'goodness', 'nervous', 'dependence', 'fail', 'ordered', 'accomplished', 'failed', 'liberal', 'clock', 'tempered', 'grave', 'civilities', 'cheerfully', 'fears', 'inclined', 'unjust', 'alarming', 'prevented', 'preference', 'unfortunate', 'frequent', 'probability', 'fortitude', 'save', 'ireland', 'tenant', 'distinguished', 'extreme', 'engage', 'wholly', 'unnecessary', 'comfortably', 'richmond', '_i_', 'continue', 'leading', 'occasionally', 'shock', 'result', 'inconvenience', 'involved', 'parcel', 'independent', 'progress', 'later', 'compare', 'nash', 'syllable', 'strongest', 'active', 'excited', 'neighbour', '_very_', 'reserved', 'exertion', 'choice', 'increasing', 'consolation', 'scruples', 'compared', 'beloved', 'succeeded', 'forming', 'concerns', 'apologies', 'cool', 'amuse', '_not_', 'recommended', 'improvement', 'remaining', 'black', 'suddenly', 'precisely', 'peculiarly', 'sufficiently', 'visitor', 'windows', 'fallen', 'feels', 'tuesday', 'various', 'continual', 'address', 'recommendation', 'partner', 'talents', 'bloom', 'submitted', 'distressing', 'announced', 'uneasiness', 'coldness', 'invitations', 'advise', 'heat', 'selina', 'plans', 'coolly', 'language', 'happiest', 'board', 'speaks', 'hurried', 'rejoice', 'introduce', 'brunswick', 'composed', 'crowd', 'watched', 'symptoms', 'paying', 'merits', 'blind', 'alarmed', 'observations', 'schemes', 'awkward', 'smallridge', 'farmer', 'twelve', 'regrets', 'humoured', \"!'\", 'resources', 'greatly', 'establishment', 'resolve', 'engaging', 'momentary', 'convenience', 'simplicity', 'nearer', 'occupy', 'trusted', 'expense', 'overcome', 'performance', 'propriety', 'require', 'deceived', 'across', 'excuses', 'anxiously', 'improve', 'brain', 'finished', 'principally', 'excite', 'danced', 'surprised', 'patty', 'accepting', 'mill', 'decision', 'missed', '_him_', 'accordingly', 'mutual', 'unreasonable', 'instance', 'aloud', 'trial', 'dr', 'liking', 'arms', 'positively', 'animated', 'objects', 'amusing', 'encouraged', 'size', 'sink', 'inquire', 'prospects', 'loves', 'suited', 'zeal', 'recovering', 'raised', 'attraction', 'nursery', 'distressed', 'sweetness', 'frequently', '_me_', 'turns', 'affectionate', 'arrive', 'heavy', 'inconvenient', 'displeased', 'applied', 'professed', 'sincerely', 'unwilling', 'guided', 'suggested', 'weak', 'resist', 'distinct', 'roused', 'cheerfulness', 'depended', 'war', 'stopt', 'injury', 'forgive', 'formerly', 'preparing', 'union', 'suspecting', 'correct', 'owed', 'apology', 'ay', 'recollections', 'apparently', 'denying', 'delightfully', 'wondering', 'class', 'fix', 'guessed', 'writes', 'prosperity', 'reconciliation', 'temptation', 'awkwardness', 'broken', 'complaint', 'curate', 'surry', 'fearful', 'mamma', 'female', 'ample', 'approached', 'adjoining', 'cake', 'blunder', 'unfit', 'unfeeling', 'disparity', 'essential', 'previously', 'indignation', 'risk', 'hoping', 'induce', 'september', 'cross', 'employ', 'endured', 'blushed', 'escaped', 'named', 'scrupulous', 'afforded', 'kingston', 'tete', 'latter', 'furniture', 'restored', 'lower', 'perceived', 'existence', 'fifty', \"?'\", 'continuing', 'principle', 'shirley', 'theirs', 'sinking', 'calm', 'resentment', 'animation', 'pressed', 'heaven', 'quickness', 'youngest', 'forgiven', 'spread', 'reconciled', 'distinctly', 'hastily', 'shook', 'parents', 'jealousy', 'discovery', 'chosen', 'quitting', 'shocked', 'belonged', 'gardens', 'meetings', 'valued', 'plenty', 'unwelcome', 'moreover', 'distinguish', '.\\'\"', 'cordiality', 'accounts', 'behaved', 'oppose', 'cox', 'prevailed', 'paused', 'alike', 'previous', 'wingfield', 'disgust', 'unwell', 'governess', 'tranquillity', 'plaister', 'shocking', 'baronet', 'winthrop', 'exert', 'difficult', 'spared', 'sincere', 'gruel', 'sixteen', 'yorkshire', 'exploring', 'included', 'informed', 'belonging', 'doubts', 'attempted', 'wet', 'necessarily', 'operation', 'june', 'unworthy', 'sailors', 'travelling', 'talent', 'strongly', 'confused', 'baked', 'weight', 'decent', 'concealment', 'amiss', 'improper', 'cautious', 'dined', 'saturday', 'wealth', 'devoted', 'unexceptionable', 'bragge', 'thousands', 'careful', 'tears', 'running', 'pounds', 'chaise', 'assisted', 'singing', 'softened', 'anticipated', 'modern', 'acquired', 'recover', 'remark', 'honoured', 'reflections', 'pair', 'declined', 'marked', 'unequal', 'visitors', 'neat', 'warmest', 'deeply', 'selfishness', 'requires', 'bit', 'remembered', 'mentioning', 'troublesome', 'discussion', 'related', 'fever', 'lessen', 'questioned', 'additional', 'powerful', 'spending', 'pursuits', 'visiting', 'approved', 'explained', 'unlike', 'wiser', 'consulted', 'footing', 'scenes', 'partial', 'justified', 'lessened', 'laconia', 'ascertain', 'actual', 'bristol', '.)', 'unfortunately', 'grief', 'calmness', 'crossed', 'gay', 'described', 'matrimony', 'gentleness', 'succeeding', 'mischief', 'sincerity', 'imprudence', 'somewhere', 'intelligent', 'constancy', 'prefer', 'addressed', 'complaisance', 'finish', 'wherever', 'wondered', 'relative', 'careless', 'tiresome', 'officer', 'established', 'approaching', 'drove', 'midst', 'misfortune', 'enjoyed', 'conclude', 'treated', 'afternoon', 'jealous', 'expressing', 'procure', 'curious', 'accommodation', 'complain', 'appearing', 'strawberries', 'thankful', 'upright', 'struggle', 'finest', 'describe', 'announce', 'damp', 'intentions', 'chiefly', 'invalid', 'suitable', 'professions', 'reception', 'cards', 'arrangements', '_we_', 'hurrying', 'inviting', 'joke', 'entitled', 'directed', 'stairs', 'exultation', 'promises', 'guests', 'praised', 'dwelt', 'implied', 'appearances', 'pen', 'ungrateful', 'acceptable', 'opposing', 'lover', 'leaning', 'surely', 'hereafter', 'learnt', 'disinterested', 'openly', 'acquit', 'embarrassed', 'talks', 'absolute', 'declaration', 'thoughtful', 'remembering', 'bred', 'gratification', 'polite', 'exclaiming', 'cordial', 'contrived', 'seventeen', 'attach', 'finery', 'ruin', 'leg', 'steadiness', 'deficient', 'prudent', 'abominable', 'applications', 'gloves', 'apprehension', 'thick', 'irish', 'kindest', 'sophy', 'lace', 'westons', 'communications', 'vexation', 'provoking', 'attack', 'possessed', 'pressing', 'design', 'perception', 'hospitality', 'dressed', 'situations', 'foreseen', 'uncertain', 'plea', 'frightful', 'blessings', 'westgate', 'cleared', 'grandeur', 'unable', 'presumption', 'relieved', 'refusal', 'deserted', 'likewise', 'chat', 'sanguine', 'declining', 'enquiries', 'reverse', 'admiring', 'retired', 'raising', 'recollected', 'decide', 'consult', 'utter', 'removing', 'dances', 'blindness', 'materially', 'drawback', 'generosity', 'whispered', 'windsor', 'gaiety', 'arriving', 'rapid', 'filled', 'parade', 'conceal', 'freckles', 'deserving', 'oftener', 'regretted', 'blushing', 'wind', 'slowly', 'shade', 'approve', 'entreated', 'recollecting', 'agony', 'born', 'tall', 'blunders', 'walks', 'sufferings', 'offers', 'estimate', 'prompt', 'settling', 'appeal', 'ear', 'nineteen', 'resolutely', 'firm', 'spectacles', 'sympathy', 'talker', 'principles', 'driving', 'irritation', 'warmer', 'barely', 'knightleys', 'modest', 'harp', 'acquiescence', 'congratulations', 'solicitous', 'february', 'glowing', 'resumed', 'reaching', 'mansion', 'submit', 'provided', 'alive', 'cutting', 'urge', 'abundance', 'natured', 'barouche', 'asp', 'dearer', 'perseverance', 'sensibility', 'alter', 'relations', 'indulge', 'joint', 'conceive', 'motives', 'presumed', 'perpetual', 'bateses', 'belief', 'embarrassment', 'mental', 'exciting', 'positive', 'destroyed', 'accompanied', 'punctually', 'shaken', 'pointed', 'usefulness', 'recommendations', 'clergyman', ';--\"', 'explain', 'wholesome', 'extensive', 'gentility', 'sweep', 'example', 'sacrifices', 'consulting', 'rising', 'abruptly', 'quantity', 'slightest', 'destiny', 'model', 'pencil', 'resident', 'coachman', 'deserves', 'relate', '.--`', 'edward', 'unusual', 'pleasanter', 'witnessed', 'portion', 'compassionate', 'convenient', 'judgement', 'gladly', ').', 'group', 'monkford', 'exist', 'display', 'prose', 'unwholesome', 'correspondent', 'gained', 'distinguishing', 'prudence', 'enjoying', 'flow', 'happening', 'marlborough', 'witness', 'wives', 'handed', 'incessant', 'sized', 'cordially', 'welcomed', 'prosperous', 'division', 'entertaining', 'rapidly', 'injustice', 'arise', 'confirmed', 'december', 'taunton', 'plymouth', 'expectations', 'cruel', 'sorrowful', 'volume', 'hopeless', 'sighed', 'surrounded', 'candour', 'tendency', 'breathe', 'attendance', 'promising', 'concealing', 'defer', 'entreaties', 'hesitated', 'exertions', 'unlucky', 'indebted', 'killed', 'twelvemonth', 'bottom', '_one_', 'madness', 'strengthened', 'detained', 'daring', 'obtained', 'renewed', 'contemplation', 'graciously', 'unaffected', 'disturbed', 'calculated', 'introducing', 'shooting', 'w', 'sofa', 'enquired', 'voluntarily', 'forbearance', 'indulged', 'entertained', 'avoiding', 'eligible', 'application', 'safer', 'original', 'deficiency', 'avoided', 'variety', 'comparatively', 'losing', 'spruce', '--`', 'refrain', 'betrayed', 'confident', 'friendliness', 'suits', 'glimpse', 'game', 'amongst', 'envy', 'degradation', 'degrading', 'learned', 'press', 'joyous', 'insensible', 'richard', 'alloy', 'numerous', 'relation', 'believing', 'inevitable', 'allowances', 'bewitching', 'trick', 'unexpected', 'formal', 'lessening', 'naval', 'apple', 'despair', 'upper', 'solitary', 'collecting', 'indies', 'meanwhile', 'manage', 'persuading', 'dissuade', 'retirement', 'mixed', 'earliest', 'activity', 'coxes', 'bathing', 'constitution', 'elliots', 'bowed', 'grandmother', 'sickness', 'recent', 'uncomfortable', 'weakness', 'fashioned', 'uttered', 'ceaseless', 'bank', 'congratulate', 'accommodations', 'voices', 'speedily', 'communicate', 'healthy', 'penance', 'total', 'rejoiced', 'imprudent', 'treatment', 'experience', 'similar', 'native', 'beds', 'amends', 'granted', 'unreserve', 'injured', 'hears', 'handsomely', 'startled', 'landau', 'imaginary', 'endeavoured', 'younger', 'yards', 'discovered', 'acknowledgment', 'affect', 'travel', 'system', 'gravely', 'yield', 'influenced', 'mild', 'dependent', 'misconduct', 'glancing', 'assisting', '_he_', 'irresistible', 'insufferable', 'recovery', 'taught', 'decisive', 'discoveries', 'plainly', 'proposals', 'supplied', 'discern', 'rooke', 'resolving', 'events', 'uncertainty', 'ostler', 'discerned', 'wright', 'hastened', 'utility', 'calls', 'somehow', 'rousing', 'displeasure', 'listener', 'butcher', 'interchange', 'inferred', 'worn', 'expose', 'beings', 'woodhouses', 'pardoned', 'varieties', 'security', 'unnatural', 'burn', 'quarrelling', 'resignation', 'draught', 'kingdom', 'behave', 'addresses', 'drawings', 'alluded', 'somersetshire', '!--(', 'completed', 'unpersuadable', 'contemptible', 'marries', 'centre', 'elsewhere', 'jemima', 'thirteen', 'papers', 'seas', 'adopt', 'reflected', 'hysterical', 'concealed', 'promoted', 'respectability', 'astley', 'disgrace', 'mystery', 'nodding', 'occasioned', 'butler', 'advising', 'deceive', 'guessing', 'emotions', 'subdued', 'explanations', 'goodwill', 'enabled', 'steadily', 'maintaining', 'spirited', 'lent', 'contained', 'alertness', 'rapidity', 'affair', 'scattered', 'impressed', 'unseen', 'fashionable', '_them_', 'curtains', 'bench', 'practicable', 'apartment', 'compose', 'qualities', 'strangers', 'boot', 'depressed', 'treachery', 'dick', 'rejected', 'maintained', 'privilege', 'everywhere', 'january', 'permitted', 'unsuspicious', 'fatigued', 'repeatedly', 'released', 'charades', 'intently', 'resemblance', 'agreeably', 'misled', 'refresh', 'chief', 'intending', 'blue', 'unreasonably', 'unnecessarily', 'apprehensive', 'throughout', 'michaelmas', 'allowable', 'pleases', 'remarkable', 'educated', 'analogy', 'checked', 'deference', 'remains', 'uncommon', 'roast', 'pavement', 'precedence', 'evenings', 'approving', 'brilliant', 'holidays', 'fourteen', 'raptures', 'aid', 'astonishing', 'humph', \";'\", 'sailor', 'delays', 'reference', 'bestowed', 'unlikely', 'hesitate', 'accompany', 'admirer', 'seclusion', 'knocked', 'provoked', 'devotion', 'exclamation', 'beforehand', 'rumour', 'suggestions', 'welfare', 'excusable', 'replying', 'quarrelled', 'overpowered', 'notion', 'inferiority', 'marking', 'nursed', 'whispering', 'stupid', 'indubitable', 'alacrity', 'pitiful', 'downright', 'hearty', 'proportions', 'stokes', 'disagree', 'graceful', 'effort', 'reminded', 'originally', 'suggest', 'impossibility', 'repetition', 'widower', 'recently', 'commonplace', 'owned', 'allowing', 'stroll', 'county', 'caring', 'gentlemanlike', 'moderately', 'hayters', 'slave', '_just_', 'preserve', 'conjecture', 'opportunities', 'pursuit', 'serve', ';\"--', '_my_', 'deplorable', 'refinement', 'justify', 'esq', 'rules', 'fixing', 'applying', 'examined', 'recommending', 'fondly', 'noticing', 'candles', 'boiled', 'averted', 'contrive', 'source', 'comprehending', 'wretchedness', 'crowded', 'considers', 'assembled', 'confessing', 'park', 'fellows', 'reflect', 'abilities', 'qualified', 'benevolent', 'quarters', 'capital', 'comforted', 'poetry', 'friday', 'irresolute', 'excused', 'rightly', 'punishment', 'announcing', 'inequality', 'lawn', 'delusion', 'disdain', 'dalrymples', 'solemn', 'folding', 'apprehend', 'hodges', 'trembling', 'restraints', 'prevail', 'explore', 'sucklings', 'interview', 'representation', 'favourably', 'pretensions', 'committed', 'freedom', 'reluctance', 'satisfactory', 'custom', 'pays', 'partiality', 'gratifying', 'perplexity', 'airing', 'baldwin', 'row', 'wrapt', 'objections', 'matches', 'handwriting', 'artificial', 'indispensable', 'supposition', 'mortifying', 'protested', 'conceived', 'impressions', 'gain', 'footpath', 'politely', 'started', 'enquiry', 'relationship', 'repeating', 'hospitable', 'accomplishments', 'deaf', 'esteemed', 'respected', 'series', 'collect', 'fruit', 'adding', 'gilbert', 'cheer', 'page', 'tranquil', 'pleasantest', 'letting', 'topic', 'decorum', 'inevitably', 'complaints', 'engrossed', 'connection', 'gown', 'insult', 'concurrence', 'ii', 'observant', 'characters', 'breaking', 'basin', 'limited', 'claimed', 'balls', 'designs', 'pushed', 'dinners', 'separated', 'bearing', 'counsel', 'securing', 'rivet', 'detail', 'selfish', 'topics', 'allusion', 'whoever', 'ridiculous', 'thanked', 'privy', 'contempt', 'subsequent', 'hamilton', 'entertain', 'liberality', 'oblige', 'ushered', 'process', 'asserted', 'flutter', 'curricle', 'comparing', 'shrubberies', 'date', 'reward', 'arrange', 'george', 'openness', 'penetration', 'meadows', 'succeed', 'suppressed', 'jump', 'producing', 'deemed', 'reckoned', 'measures', 'amidst', 'provide', 'ceremonious', 'speculation', 'miserably', 'grieving', 'ungracious', 'livery', 'denial', 'formidable', 'rivers', 'liveliness', 'perturbation', 'negative', 'kitty', 'frozen', 'mutually', 'retrench', 'fairy', 'copied', 'unsuitable', '_the_', 'blank', 'dressing', 'calculations', 'unattended', 'humourist', 'supplying', 'requisite', 'hesitatingly', 'deplore', 'ingenuity', 'unnoticed', 'clearly', '_is_', 'extravagant', 'unaccountable', 'chuses', 'witnessing', 'homes', 'unpardonable', 'unkind', 'maintenance', 'bewildered', 'grandpapa', 'freshness', 'sources', 'ardent', 'heroism', 'apothecary', 'coxcomb', 'using', 'shamefully', 'sequel', 'smoothness', 'probabilities', 'declares', 'moral', 'discover', 'abrupt', 'behaving', 'advisable', 'rencontre', 'condolence', 'overheard', 'poverty', 'shilling', 'reappeared', 'youthful', 'airs', 'pert', 'climate', 'ballroom', 'creditable', 'perrys', 'fearless', 'obscurity', 'mortifications', 'device', 'urgent', 'breathed', 'stepping', 'plants', 'discreet', 'pointing', 'attentively', 'withdrawn', 'soothing', 'renewal', 'privileges', 'tenderest', 'augusta', '10', 'accidental', 'dutiful', 'requiring', 'longest', 'nursing', 'shoes', 'firmly', 'floor', 'gratefully', 'neglecting', 'preceding', 'pitiable', 'truths', 'orders', '_your_', '_will_', 'shore', 'bitterly', 'colds', 'procuring', 'welcoming', 'articles', 'unfairly', 'mutton', 'proposition', 'naming', 'assuring', 'loin', 'dropped', 'cleverer', 'longing', 'fetching', 'shrubbery', 'feelingly', 'club', 'wainscot', 'pronounced', 'calmly', ',--\"', 'disordered', 'poignant', 'pushing', 'happens', 'judicious', 'seats', 'measles', 'pitied', 'trusting', 'feet', 'reality', 'milsom', 'phoo', 'solitude', 'seemingly', 'partridge', 'irritated', 'inquiring', ':\"', 'kinder', ':--\"', 'x', 'bought', 'stept', 'beneath', 'descriptions', 'preserved', 'inconstancy', 'sands', 'forest', 'consequences', 'considerably', 'failure', 'entangled', 'viii', 'promote', 'july', 'premature', 'disgraced', 'prime', 'wear', 'examining', 'nephews', 'grows', 'disappointments', 'solicitudes', 'comprehension', 'thoughtless', 'lessons', 'affectionately', 'forlorn', 'minutiae', 'relenting', 'exposed', 'shameful', 'insolent', 'blameless', 'elegantly', 'saved', 'individual', 'acquittal', 'walker', 'occupying', 'loudly', 'overhearing', 'stile', 'horseback', 'ascertained', 'nut', 'outlived', 'insensibility', 'amount', 'reports', 'gravel', 'serle', 'indisposed', 'proposing', 'universally', 'providence', 'playful', 'infection', 'robinson', 'confinement', 'recall', 'shabby', 'sole', 'nonsensical', 'overpowering', ');', 'deeper', 'usually', 'genteel', 'narration', 'pursued', 'glances', 'vigorously', 'lists', 'newspapers', 'cases', 'dissipated', 'shelter', 'gibraltar', 'residing', 'intimates', 'saving', 'astray', 'affronted', 'boasted', 'unfair', 'delicious', 'stretching', 'situated', 'absenting', 'unqualified', 'earnestness', 'rejoined', 'covered', 'terror', 'cheap', 'conveniently', 'preventing', 'apprehensions', 'females', 'reigns', \"'--\", 'perpetually', 'hardship', 'grieve', 'unheard', 'bordered', 'amazing', 'unpretending', 'apart', 'counteract', 'driven', 'harmless', 'occurrence', 'intervals', 'iv', 'misunderstandings', 'asparagus', 'gowland', 'prettily', 'creating', 'preferring', 'musician', 'loveliness', 'harmony', 'xi', 'charmingly', 'social', 'nearest', 'voluntary', 'ships', 'destination', 'fastidious', 'detected', 'inducement', 'joyful', 'local', 'confirmation', 'compatible', 'rained', 'shrink', 'explains', 'excessive', 'projected', '_now_', 'riddle', 'ribbon', 'declaring', 'unwillingness', 'hesitating', 'estimation', 'october', 'persevered', 'disclosure', 'teachers', 'moderation', 'tidings', 'describing', 'trivial', 'reverie', 'intimately', 'smaller', 'services', 'august', 'owing', 'sins', 'dowager', 'viscountess', 'dealings', 'modes', 'augur', 'restrictions', 'ruined', 'practised', 'v', 'xiv', 'shawl', 'considerate', 'penetrating', 'staircase', 'finally', 'economy', 'employing', 'persisting', 'correspondence', 'helpless', 'romance', 'storm', 'enquiring', 'maintain', 'piano', 'flower', 'remove', 'efficacy', 'proofs', 'clearing', 'treat', 'divide', 'genuine', 'expenses', 'regretting', 'parent', 'calmer', 'enjoyments', 'spoiled', 'observance', 'examination', 'recur', 'branch', 'vast', 'misunderstood', 'alteration', 'denied', 'suspicious', 'dreaded', 'quivering', 'lip', 'changing', 'contemplate', 'handsomest', 'hereabouts', 'hedges', 'complacency', 'disappoint', 'principals', 'meal', 'noticed', 'unknown', 'agitations', 'respecting', 'hardened', 'waiter', 'pointedly', 'softness', 'seize', 'shropshire', 'revived', 'reluctant', 'mix', '_his_', 'presumptive', 'resigned', 'requested', 'extravagance', 'bonnet', 'wallises', 'gratify', 'limits', 'inquired', 'sixty', 'impulse', 'likelihood', 'ix', 'grievance', 'quicker', 'animating', 'silenced', 'encumbrance', 'affectedly', 'string', 'sketch', 'clearer', 'umbrellas', 'clearness', 'stupidity', 'dulness', 'airy', 'forms', 'foresaw', 'elevate', 'humouredly', 'curacy', 'readiness', 'advanced', 'doubtingly', 'basil', 'felicities', 'iii', 'sentences', 'heavens', 'delivered', 'incommoded', 'congratulated', 'unite', 'unmanageable', 'naivete', 'eloquent', 'persuadable', 'bows', 'interests', 'energy', 'occasional', 'preparatory', 'jumped', 'suddenness', 'glass', 'convictions', 'unquestionably', 'pondered', 'consoling', 'furnished', 'attachments', 'standard', 'disgusting', 'novelty', 'swisserland', 'associate', 'doubly', 'omission', 'stable', 'xii', 'gifted', 'attractions', 'virtues', 'popularity', 'thanking', 'yard', 'expressive', 'vi', 'retract', 'whims', 'vex', 'accomplishment', 'needless', 'departure', 'xiii', 'seized', 'scholar', 'boarder', 'limbs', 'heightened', 'protection', 'including', 'xvii', 'arch', 'artist', 'compliance', 'suspension', 'tones', 'demure', 'remonstrance', 'venturing', 'charms', 'indignant', 'contrast', 'yielding', 'cured', 'comprehended', 'conversable', 'usage', 'composedly', 'wickedness', 'cow', 'prominent', 'regularly', 'yarmouth', 'absurdity', 'snowing', 'foresee', 'viewed', 'exerting', 'concluding', 'accidentally', 'cooler', 'charmed', 'lame', 'convincing', 'undertakes', 'sharp', 'fireside', 'instinctively', '_must_', 'examples', 'divisions', 'luxurious', 'connect', 'renewing', 'animate', 'luckiest', 'stories', 'proportion', 'soften', 'contradict', 'palpably', 'visible', 'humiliation', 'owes', 'changes', 'draper', 'involving', 'eternal', 'privations', 'appointed', 'proving', 'quickest', 'pretended', 'vacant', 'lamenting', '_all_', 'contradiction', 'fortunately', 'rode', 'noisy', 'placing', 'attractive', 'hating', 'disorder', 'plays', 'striving', 'awful', 'guidance', 'immense', 'flying', 'laura', 'tolerate', 'destined', '_to_', '_more_', 'partake', 'affords', 'associations', 'authorised', 'coffee', 'depending', 'whist', 'rarely', 'downstairs', 'accord', 'interruption', 'chatty', 'midsummer', 'achieved', 'trunk', '_', '_elton_', 'cart', 'communicating', 'pained', 'dears', 'adieus', 'inconstant', 'indulging', 'speculations', 'frigate', 'understands', 'discipline', 'deduction', 'unsuspected', 'physician', 'atmosphere', 'arisen', 'accompanying', 'foreign', 'warfare', 'newspaper', 'perceptible', 'simpleton', 'untainted', 'languor', 'dispel', 'gloomy', 'extenuation', 'hind', 'freshened', 'easier', 'surrounding', 'tacitly', 'innocently', 'lengths', 'conundrum', 'repulsive', 'attract', 'eleven', 'fling', 'faster', 'clownish', 'joining', 'publications', 'scrape', 'hue', 'represent', 'recalled', 'permanently', 'dignified', 'graciousness', 'fagged', 'acknowledgement', 'schoolfellow', 'reduced', 'monarch', 'infatuation', 'failings', 'upstart', 'pretension', 'prescribed', 'instrumental', 'thankfulness', 'lowering', 'supports', 'involve', 'secrecy', 'connecting', 'parentage', 'revealed', 'puppy', 'fruitless', 'contrivances', 'representing', 'exposing', 'derive', 'deprecated', 'studiously', 'testify', 'varying', 'insinuating', 'afloat', 'stock', 'realised', 'preserves', 'unconsciously', 'attributing', 'summon', 'ridden', 'poorly', 'gout', '000', 'glorious', 'green', 'brown', 'richly', 'sweetly', 'serviceable', 'expedients', 'pleasantness', 'borrowed', 'scissors', 'appropriated', 'powered', 'falsehood', 'yellow', 'admires', 'muffin', 'removals', 'inconsistent', 'ajar', 'knitting', 'overpower', 'preceded', 'durable', 'finances', 'overtaken', 'huswife', 'apologise', 'pages', 'indignantly', 'disengage', 'elevation', 'desert', 'exchanged', 'estrangement', '_little_', 'honestly', 'embarrassments', 'cruelty', 'impropriety', 'equipped', 'strictly', 'imparted', 'hinting', 'arguments', 'completion', 'tied', 'contentment', 'enable', 'pacing', 'endeavouring', 'balance', 'salted', 'discerning', 'unfelt', 'sour', 'asleep', 'culture', 'earn', 'final', 'unconvinced', 'portraits', 'satin', 'overthrow', 'parishes', 'indescribable', 'incomprehensible', 'strangest', 'brighter', 'shy', 'impediment', 'amuses', 'torment', 'polished', 'ceases', 'playfulness', 'definition', 'orchestra', 'clerks', 'stays', 'nicely', 'fried', 'trimmed', 'seeking', 'abode', 'contriving', 'fearfully', 'intellectual', 'sign', 'clifton', 'closing', 'conjugal', 'states', 'fainted', '_courtship_', 'advised', 'reasoned', 'wittier', 'urged', 'flew', 'forwards', 'disagreement', 'affording', 'purchased', 'kindled', 'fatal', 'charmouth', 'cliffs', 'romantic', 'expedition', 'incumbent', 'sly', 'curtailed', 'expediency', 'grandson', 'regulations', 'reductions', 'disapprobation', 'honourably', 'tunbridge', 'dated', 'writer', 'medium', 'interfering', 'softening', 'lingering', 'courteous', 'undue', 'secondly', 'results', 'reasonably', 'lodged', 'likenesses', 'hetty', 'prized', 'singular', 'speedy', 'brigden', 'examine', 'audible', 'fetched', 'foolishly', 'conjectures', 'abused', 'follies', 'wilful', 'subduing', 'sharing', 'sobering', 'remind', 'contrition', 'flight', 'train', 'expressly', 'materials', 'glowed', 'concession', 'staring', 'extended', 'hero', 'pew', 'favouring', 'witty', 'mischievous', 'construction', 'haue', 'ham', 'caesar', 'brutus', 'bru', 'vs', 'selfe', 'thee', 'loue', 'vpon', 'heere', 'cassi', 'hor', 'hamlet', 'hath', 'giue', 'cassius', 'speake', 'antony', 'ile', 'th', 'vp', 'heare', 'doe', 'thinke', 'qu', 'looke', 'ophe', 'ant', 'feare', 'laer', 'downe', 'againe', 'heauen', 'pol', 'hee', 'leaue', 'rosin', 'owne', 'exeunt', 'queene', 'euen', 'polon', 'neuer', 'horatio', 'caes', 'hast', 'rome', 'marke', 'gods', 'liue', 'euery', 'beare', 'caesars', 'wee', 'himselfe', 'laertes', 'brut', 'caska', 'cask', 'soule', 'mar', 'deere', 'finde', 'cinna', 'meanes', 'sonne', 'ophelia', 'luc', 'lucius', 'poore', 'ghost', 'sword', 'seene', 'euer', 'selues', 'vse', 'keepe', 'clo', 'octauius', 'titinius', 'messala', 'beleeue', 'cas', 'octa', 'players', 'faire', 'bee', 'messa', 'polonius', 'vertue', 'guild', 'meane', 'sleepe', 'osr', 'worke', 'roman', 'vnto', 'backe', 'lye', 'decius', 'crowne', 'guildensterne', 'farre', 'denmarke', 'capitoll', 'madnesse', 'dye', 'thine', 'goe', 'kill', 'yong', 'lucillius', 'betweene', 'beene', 'honor', 'por', 'sicke', 'winde', 'minde', 'walke', 'sayes', 'fortinbras', 'mee', 'eares', 'romans', 'ayre', 'ouer', 'forme', 'eare', 'seeme', 'wilt', 'lookes', 'caius', 'dost', 'onely', 'loues', 'ho', 'kin', 'graue', 'meete', 'foule', 'reuenge', 'reynol', 'greefe', 'generall', 'helpe', 'newes', 'neere', 'ore', 'heauens', 'ioy', 'kinde', 'cato', 'flye', 'oft', 'metellus', 'businesse', 'drinke', 'ser', 'giuen', 'beares', 'voyce', 'pindarus', 'thrice', 'breake', 'murther', 'verie', 'gertrude', 'tooke', 'vnder', 'cicero', 'bin', 'villaine', 'turne', 'knowne', 'twere', 'giues', 'gaue', 'hell', 'portia', 'certaine', 'rosincrance', 'lesse', 'themselues', 'iudgement', 'seeke', 'dayes', 'sweare', 'marcellus', 'tane', 'beseech', 'cymber', 'guil', 'thankes', 'weepe', 'armes', 'talke', 'receiue', 'publius', 'valiant', 'lou', 'soules', 'gho', 'barnardo', 'ligarius', 'flourish', 'strato', 'trebonius', 'thanke', 'shalt', 'barn', 'poyson', 'begge', 'dreame', 'enterprize', 'weare', 'passe', 'knaue', 'halfe', 'growne', 'dreadfull', 'funerall', 'withall', 'teares', 'army', 'hora', 'cass', 'hearke', 'whil', 'ranke', 'shewes', 'hoe', 'loose', 'fat', 'volumnius', 'maiesty', 'bloody', 'stirre', 'prythee', 'beast', 'custome', 'seemes', 'braue', 'peece', 'fran', 'philippi', 'mou', 'deare', 'appeare', 'senate', 'aboue', 'ple', 'weake', 'comming', 'musicke', 'speakes', 'deliuer', 'confesse', 'houre', 'arme', 'vile', 'wherein', 'alarum', 'dagger', 'dane', 'doore', 'teare', 'proofe', 'mettle', 'wits', 'diuell', 'senators', 'slaue', 'countrymen', 'yeare', 'seuerall', 'lordship', 'sirs', 'actus', 'perceiue', 'seruice', 'wherefore', 'deci', 'lucil', 'thunder', 'obey', 'durst', 'blacke', 'seruant', 'writ', 'lacke', 'burne', 'leade', 'maiestie', 'saue', 'claudio', 'bones', 'moue', 'sodaine', 'scull', 'audience', 'twas', 'redresse', 'fauour', 'noyse', 'drowne', 'rites', 'buriall', 'obserue', 'quicke', 'norwey', 'preuent', 'iudge', 'anon', 'neede', 'manet', 'behinde', 'beware', 'yea', 'pompeyes', 'thinkes', 'wisedome', 'lepidus', 'resolu', 'fortunes', 'fie', 'wil', 'doo', 'cocke', 'fooles', 'presse', 'dumbe', 'roome', 'palme', 'alexander', 'ne', 'fellowes', 'suite', 'calphurnia', 'greefes', 'y', 'iust', 'thicke', 'dreames', 'soldiers', 'perchance', 'flauius', 'wholsome', 'mothers', 'content', 'strooke', 'tragedie', 'answere', 'sunne', 'france', 'crosse', 'hoa', 'whilst', 'shout', 'starre', 'pyrrhus', 'hamlets', 'honors', 'cursed', 'warre', 'deeds', 'mens', 'damned', 'reade', 'toward', 'knowes', 'kingdome', 'foole', 'asse', 'vowes', 'vnderstand', 'heauy', 'antonio', 'ist', 'yeares', 'monstrous', 'visage', 'nunnery', 'pulpit', 'stole', 'whereto', 'turnes', 'mindes', 'clocke', 'cruell', 'aliue', 'coward', 'ides', 'ene', 'tit', 'statue', 'liuing', 'yee', 'gowne', 'royall', 'prison', 'tent', 'yeeld', 'sleepes', 'natiue', 'freedome', 'proclaime', 'daggers', 'purposes', 'fla', 'fiery', 'walkes', 'aske', 'slaine', 'distracted', 'clit', 'dyes', 'cai', 'mur', 'tend', 'returne', 'fearefull', 'reueng', 'titin', 'naturall', 'iulius', 'bene', 'keepes', 'cob', 'distemper', 'seale', 'amisse', 'deepe', 'flowers', 'honestie', 'loe', 'var', 'shee', 'steele', 'noblest', 'sings', 'bosome', 'serues', 'locke', 'indeede', 'heerein', 'stra', 'gallowes', 'seru', 'vnfold', 'starres', 'haire', 'marcus', 'foorth', 'cries', 'followes', 'modestie', 'wounds', 'liues', 'gonzago', 'rul', 'beasts', 'gainst', 'vnkle', 'sirra', 'rash', 'spade', 'danish', 'louing', 'falles', 'sooth', 'mans', 'saide', 'feast', 'pompey', 'doores', 'mortall', 'closes', 'goodnight', 'sicknesse', 'therein', 'legions', 'beard', 'hauing', 'soone', 'pit', 'neyther', 'wide', 'recouer', 'dutie', 'clitus', 'motiue', 'traitors', 'publike', 'heeles', 'braine', 'extasie', 'ambassadors', 'valour', 'battaile', 'elsonower', 'successe', 'iephta', 'findes', 'blesse', 'choyce', 'feede', 'repaire', 'trickes', 'plaine', 'mouse', 'osricke', 'alarums', 'rosincrane', 'maine', 'proscription', 'affayres', 'alacke', 'meere', 'peepe', 'weigh', 'subiect', 'lippes', 'grone', 'lep', 'calles', 'nony', 'push', 'runne', 'leysure', 'desperate', 'tearmes', 'traine', 'cheeke', 'aduice', 'dard', 'souldier', 'cals', 'pricke', 'knocke', 'cin', 'contriue', 'pitty', 'breefely', 'glasse', 'seruants', 'powres', 'norman', 'vice', 'bondman', 'ment', 'aboord', 'saile', 'demand', 'lightning', 'deceiu', 'meerely', 'haile', 'humbly', 'flood', 'voyage', 'ee', 'spundge', 'hercules', 'liu', 'fates', 'shooes', 'strew', 'beautie', 'speciall', 'promis', 'cornelius', 'tydings', 'terrible', 'villaines', 'incestuous', 'vnnaturall', 'treb', 'norway', 'vncle', 'heares', 'vnderstanding', 'sence', 'argall', 'rous', 'expresse', 'woo', 'eate', 'olympus', 'bloud', 'tame', 'greene', 'hits', 'prouidence', 'browes', 'stuffe', 'naked', 'cal', 'ayme', 'heereafter', 'shapes', 'hew', 'aduantage', 'humor', 'prophesie', 'ope', 'vtterance', 'limbes', 'strife', 'infants', 'deede', 'groaning', 'taper', 'bap', 'murder', 'braines', 'seuen', 'stones', 'weeping', 'swords', 'lyes', 'attendants', 'wayes', 'kneele', 'fled', 'faithfull', 'dar', 'belike', 'alwayes', 'rests', 'leane', 'reynoldo', 'fret', 'wanton', 'forc', 'acte', 'carpenter', 'bleed', 'ghosts', 'prou', 'closset', 'processe', 'readie', 'varrus', 'laughter', 'signe', 'playes', 'soueraigne', 'priest', 'huge', 'wing', 'receiu', 'louer', 'angell', 'sate', 'refus', 'coniure', 'kissing', 'knees', 'yonder', 'vnlesse', 'reioyce', 'edge', 'corruption', 'sardis', 'pleas', 'loued', 'doest', 'throwne', 'necke', 'crimes', 'moone', 'generals', 'foote', 'princes', 'priuate', 'clowne', 'driue', 'hecuba', 'seal', 'slay', 'envenom', 'com', 'pate', 'quantitie', 'leaues', 'pluckt', 'maker', 'doomesday', 'philosophy', 'immortall', 'dogge', 'cowards', 'rage', 'harme', 'warlike', 'graues', 'moneths', 'growes', 'enuious', 'begger', 'compell', 'battell', 'pluck', 'wager', 'ceremonies', 'sixe', 'counsell', 'fixt', 'wonderfull', 'vow', 'siluer', 'faine', 'proue', 'neerer', 'childe', 'breed', 'hower', 'canst', 'conspirators', 'pastorall', 'historicall', 'graunt', 'suites', 'toe', 'pesant', 'courtier', 'galls', 'diadem', 'loines', 'trumpets', 'trumpet', 'drinkes', 'darke', 'rapier', 'mightie', 'tokens', 'moreouer', 'sutor', 'incorporate', 'ambitions', 'ladder', 'yeeres', 'honorable', 'firme', 'frighted', 'painted', 'sparkes', 'whereof', 'lt', 'wittingly', 'remaines', 'asleepe', 'cic', 'amaz', 'wouldest', 'loosing', 'greeke', 'dy', 'diuel', 'weaknesse', 'potent', 'gaming', 'mock', 'waxe', 'porch', 'iustly', 'lupercall', 'fits', 'grapple', 'billes', 'rises', 'deseru', 'memorie', 'fulfill', 'wheele', 'corpes', 'tending', 'split', 'vrge', 'feares', 'larded', 'importing', 'axe', 'masse', 'diuinity', 'acts', 'soyle', 'greatnesse', 'carue', 'bloodie', 'warres', 'conspiracie', 'girle', 'amaze', 'crew', 'lucianus', 'lap', 'mocke', 'stabb', 'lome', 'magots', 'pregnant', 'leape', 'schoole', 'hilts', 'cobler', 'driuen', 'morne', 'mantle', 'dew', 'yon', 'perils', 'conditions', 'vntill', 'iigge', 'knee', 'thrift', 'actor', 'remoue', 'tardie', 'greeue', 'seate', 'droppes', 'equall', 'chasticement', 'sworne', 'tents', 'maiden', 'courtiers', 'schollers', 'obseru', 'heauenly', 'canopy', 'appeares', 'pestilent', 'swore', 'streetes', 'vnckle', 'liued', 'somthing', 'pitteous', 'conuert', 'indifferently', 'weary', 'booke', 'brauery', 'mountaines', 'vilde', 'praying', 'killes', 'battailes', 'muddy', 'rascall', 'damn', 'batchellor', 'bleede', 'iustice', 'dyest', 'spurre', 'deerely', 'fierie', 'scoene', 'vtter', 'imployment', 'wag', 'ape', 'iaw', 'load', 'dishonour', 'feete', 'volt', 'recorder', 'platforme', 'seuenty', 'fiue', 'drachmaes', 'conspirator', 'sinne', 'controuersie', 'tryall', 'sucke', 'necessitie', 'niggard', 'sweete', 'hastie', 'murderer', 'cynna', 'violets', 'torrent', 'garland', 'voltumand', 'prickt', 'signifie', 'choller', 'thrusting', 'element', 'wormes', 'mistris', 'resort', 'messengers', 'bastard', 'nephewes', 'suppresse', 'whale', 'poysoner', 'loath', 'awhile', 'peeuish', 'foe', 'tyrants', 'sham', 'sinke', 'wildenesse', 'wonted', 'winters', 'mountaine', 'blew', 'eternall', 'sparke', 'vndertake', 'ioynt', 'ioyes', 'chanc', 'plebeians', 'kisse', 'shed', 'foyles', 'odde', 'voltemand', 'clownes', 'vnbraced', 'iealous', 'recount', 'plots', 'camell', 'bonds', 'commoners', 'obseruance', 'bodie', 'fierce', 'ciuill', 'italy', 'confines', 'hauocke', 'carrion', 'burnes', 'winne', 'formall', 'bury', 'euill', 'sometime', 'knauish', 'reades', 'soothsayer', 'murellus', 'heate', 'perillous', 'pronounc', 'mutiny', 'enemie', 'lyons', 'offall', 'progresse', 'weapons', 'forehead', 'ordinance', 'heau', 'sourse', 'denmark', 'wake', 'gouerne', 'tweene', 'lyon', 'throate', 'prick', 'torches', 'ioyn', 'comedie', 'whiles', 'wilde', 'octauio', 'armour', 'yond', 'theame', 'naught', 'passions', 'construe', 'blest', 'calp', 'shooke', 'drum', 'venome', 'pind', 'secrecie', 'apparition', 'cloake', 'griefe', 'birds', 'qualitie', 'natures', 'spake', 'cup', 'fye', 'sawcy', 'mistrust', 'forgiue', 'arras', 'liest', 'barke', 'slaues', 'oathes', 'scandall', 'popil', 'aduancement', 'imports', 'strangely', 'arrant', 'twelue', 'passionate', 'ros', 'plac', 'falne', 'shrunke', 'dec', 'moued', 'lust', 'celestiall', 'prey', 'vttered', 'thriue', 'straine', 'hearers', 'mischeefe', 'mars', 'yesternight', 'doublet', 'tyber', 'shores', 'metel', 'slew', 'lookt', 'vnseene', 'plucke', 'beside', 'maiesties', 'merrie', 'iot', 'cap', 'romane', 'speechlesse', 'quarrell', 'ifaith', 'inobled', 'craft', 'assur', 'scope', 'truely', 'whatsoeuer', 'primus', 'puh', 'wormwood', 'lowe', 'easinesse', 'sorrie', 'cÃ¦sar', 'iealousie', 'mess', 'chide', 'whereon', 'fee', 'poleak', 'pole', 'enact', 'replication', '].', 'els', 'sticke', 'sufferance', 'windowes', 'beating', 'rogue', 'drift', 'treacherous', 'sober', 'fals', 'attaine', 'rew', 'kil', 'millions', 'burning', 'cheere', 'discomfort', 'womens', 'actors', 'crowes', 'election', 'discouer', 'swet', 'grownd', 'wauing', 'brands', 'harlot', 'itching', 'mart', 'windes', 'roughly', 'twixt', 'twaine', 'byrlady', 'hoby', 'courtesie', 'tenders', 'shell', 'cryed', 'rend', 'brooke', 'streame', 'liberall', 'weeds', 'spred', 'pul', 'guts', 'farwell', 'pious', 'popillius', 'enuy', 'disclos', 'stretcht', 'gray', 'beards', 'witnesse', 'serue', 'vantage', 'witchcraft', 'grosse', 'charme', 'foure', 'houres', 'crack', 'pith', 'ancestors', 'belou', 'spectacle', 'peeces', 'terme', 'anticke', 'deckt', 'steale', 'cannon', 'hangers', 'moneth', 'yoake', 'couch', 'hounds', 'shal', 'accidentall', 'brest', 'termes', 'pastime', 'wel', 'auoyd', 'whet', 'serpent', 'storme', 'region', 'nightly', 'toyles', 'affrighted', 'clap', 'blasted', 'iudgements', 'forgetfull', 'stab', 'breefe', 'liege', 'shrewdly', 'mutes', 'saies', 'strawes', 'assay', 'gall', 'blowne', 'heauie', 'purging', 'worthinesse', 'vnderneath', 'builds', 'mason', 'shipwright', 'shold', 'ensigne', 'whit', 'distract', 'rapiers', 'bondage', 'visitation', 'flint', 'parchment', 'skinnes', 'tride', 'ripe', 'ioyne', 'flash', 'rotten', 'horrid', 'bleeding', 'butchers', 'priam', 'imperiall', 'blowes', 'vnknowne', 'councell', 'rebellious', 'louers', 'wast', 'kites', 'cerimony', 'fantasie', 'pawse', 'trash', 'scourge', 'speede', 'toy', 'guildenstern', 'stomacke', 'mourn', 'tyrant', 'griefes', 'pittie', 'comicall', 'tragicall', 'beguile', 'buzze', 'strucke', 'mooue', 'fauours', 'keene', 'interim', 'testament', 'dardanius', 'stoope', 'bribes', 'ophel', 'passeth', 'trappings', 'carde', 'equiuocation', 'vndoe', 'picked', 'kibe', 'threatning', 'bisson', 'rheume', 'clout', 'lanke', 'teamed', 'blanket', 'villany', 'cups', 'kettle', 'cannoneer', 'cannons', 'cauerne', 'maske', 'antike', 'heraulds', 'shelfe', 'coynage', 'bodilesse', 'drownes', 'blench', 'ioyfully', 'lowlynesse', 'climber', 'vpward', 'attaines', 'vpmost', 'scorning', 'ascend', 'til', 'priests', 'stayes', 'physicke', 'prolongs', 'dispos', 'seduc', 'israel', 'greekes', 'madman', 'lyest', 'skies', 'vnnumbred', 'vnknowing', 'circumscrib', 'yeelding', 'bargaine', 'iuggel', 'rant', 'offendendo', 'argues', 'ro', 'rebellion', 'gyant', 'meate', 'earnes', 'southerly', 'hawke', 'handsaw', 'apron', 'shouted', 'trap', 'melancholly', 'abuses', 'damne', 'menace', 'pretors', 'chayre', 'hoorded', 'extorted', 'wombe', 'woodcocke', 'sprindge', 'treacherie', 'kingly', 'beauer', 'cum', 'alijs', 'sect', 'boorded', 'cleare', 'shippe', 'outlarie', 'smelt', 'yorick', 'iest', 'gorge', 'vntrod', 'validitie', 'fruite', 'vnripe', 'stickes', 'vnshaken', 'mellow', 'purpled', 'reeke', 'smoake', 'speedier', 'smels', 'primall', 'marrie', 'bang', 'proceede', 'sutors', 'accoutred', 'recame', 'offends', 'robustious', 'pery', 'wig', 'pated', 'tatters', 'ragges', 'groundlings', 'capeable', 'inexplicable', 'whipt', 'termagant', 'outherod', 'herod', 'petitions', 'cabin', 'scarft', 'grop', 'withdrew', 'vnseale', 'knauery', 'denmarks', 'englands', 'hoo', 'bugges', 'goblins', 'superuize', 'leasure', 'bated', 'grinding', 'foolerie', 'digest', 'venom', 'spleene', 'scholler', 'cautell', 'besmerch', 'vnuallued', 'sanctity', 'iumpe', 'polake', 'arriued', 'affabilitie', 'erebus', 'dimme', 'preuention', 'feauer', 'spaine', 'lustre', 'bookes', 'feeble', 'maiesticke', 'sorrowes', 'spies', 'battalians', 'ladie', 'insupportable', 'losse', 'quake', 'returneth', 'conuerted', 'stopp', 'beere', 'barrell', 'quintus', 'reueale', 'ouerlook', 'fac', 'raines', 'doue', 'tarry', 'prethee', 'whilest', 'maisters', 'amb', 'cimber', 'preferre', 'calender', 'challenger', 'chidden', 'ferret', 'crost', 'oration', 'russet', 'clad', 'easterne', 'abler', 'oct', 'signall', 'flashes', 'rore', 'baudry', 'barre', 'libertie', 'candied', 'pompe', 'crooke', 'hindges', 'faining', 'rerule', 'goodman', 'deluer', 'salutation', 'beckens', 'rossius', 'fishmonger', 'bait', 'falshood', 'windlesses', 'assaies', 'indirections', 'tyrannie', 'muddied', 'vnwholsome', 'whispers', 'greenly', 'hugger', 'mugger', 'interre', 'vnskilfull', 'iudicious', 'reway', 'theater', 'globe', 'ruddy', 'buffets', 'nunnerie', 'showts', 'clamors', 'maintains', 'nutshell', 'sodainely', 'pub', 'plucking', 'intrailes', 'feele', 'infaith', 'mood', 'scanter', 'entreatments', 'expectansie', 'mould', 'obseruers', 'inuites', 'discouery', 'secricie', 'moult', 'feather', 'forgone', 'sterrill', 'promontory', 'maiesticall', 'roofe', 'fretted', 'golden', 'congregation', 'vapours', 'drawne', 'heape', 'gastly', 'mowes', 'ducates', 'sterne', 'exits', 'saint', 'patricke', 'remaster', 'reform', 'arriu', 'tenure', 'cutpurse', 'empire', 'brau', 'con', 'roate', 'sayst', 'tut', 'bosomes']\n"
     ]
    }
   ],
   "source": [
    "def unique_vocabulary(label1, label2, cutoff):\n",
    "    voc1 = []\n",
    "    voc2 = []\n",
    "    for (sent, label) in strat_train_set:\n",
    "        if label==label1:\n",
    "            for word in sent:\n",
    "                voc1.append(word.lower())\n",
    "        elif label==label2:\n",
    "            for word in sent:\n",
    "                voc2.append(word.lower())    \n",
    "    counts1 = Counter(voc1)\n",
    "    sorted_counts1 = sorted(counts1.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    counts2 = Counter(voc2)\n",
    "    sorted_counts2 = sorted(counts2.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    unique_voc = []\n",
    "    for i in range(0, round(len(sorted_counts1)*cutoff)):\n",
    "        if not sorted_counts1[i][0] in counts2.keys():\n",
    "            unique_voc.append(sorted_counts1[i][0])\n",
    "    for i in range(0, round(len(sorted_counts2)*cutoff)):\n",
    "        if not sorted_counts2[i][0] in counts1.keys():\n",
    "            unique_voc.append(sorted_counts2[i][0])\n",
    "    return unique_voc\n",
    "    \n",
    "unique_voc = unique_vocabulary(\"austen\", \"shakespeare\", 0.5)\n",
    "print(len(unique_voc))\n",
    "print(unique_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_counts(text, unique_voc):\n",
    "    unique_counts = {}\n",
    "    words = []\n",
    "    for word in text:\n",
    "        words.append(word.lower())\n",
    "    counts = Counter(words)\n",
    "    for word in unique_voc:\n",
    "        if word in counts.keys():\n",
    "            unique_counts[word] = counts.get(word)\n",
    "        else: unique_counts[word] = 0\n",
    "    return unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 13414\n",
      "3354 3354\n",
      "6906 6906\n",
      "\n",
      "0.9594514013118665\n",
      "[[2231   69]\n",
      " [  67  987]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      2300\n",
      "          1       0.93      0.94      0.94      1054\n",
      "\n",
      "avg / total       0.96      0.96      0.96      3354\n",
      "\n",
      "0.9561251086012164\n",
      "[[4867  132]\n",
      " [ 171 1736]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97      4999\n",
      "          1       0.93      0.91      0.92      1907\n",
      "\n",
      "avg / total       0.96      0.96      0.96      6906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_dataset(source, source_docs):\n",
    "    all_features = []\n",
    "    targets = []\n",
    "    for (sent, label) in source:\n",
    "        feature_list=[]\n",
    "        feature_list.append(avg_number_chars(sent))\n",
    "        feature_list.append(number_words(sent))\n",
    "        counts = word_counts(sent)\n",
    "        for word in STOP_WORDS:\n",
    "            if word in counts.keys():\n",
    "                feature_list.append(counts.get(word))\n",
    "            else:\n",
    "                feature_list.append(0)        \n",
    "        feature_list.append(proportion_words(sent, STOP_WORDS))\n",
    "        p_counts = pos_counts(sent, source_docs, pos_list)\n",
    "        for pos in p_counts.keys():\n",
    "            feature_list.append(float(p_counts.get(pos))/float(len(sent)))\n",
    "        s_counts = suffix_counts(sent, source_docs, selected_suffixes)\n",
    "        for suffix in s_counts.keys():\n",
    "            feature_list.append(float(s_counts.get(suffix))/float(len(sent)))\n",
    "        u_counts = unique_counts(sent, unique_voc)\n",
    "        for word in u_counts.keys():\n",
    "            feature_list.append(u_counts.get(word))\n",
    "        all_features.append(feature_list)\n",
    "        if label==\"austen\": targets.append(0)\n",
    "        else: targets.append(1)\n",
    "    return all_features, targets\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final and best result: $0.9598$ on the pretest and $0.9584$ on the test set â€“ i.e., almost identical! What is more, the performance on both classes, majority as well as minority, is now also almost identical â€“ F1 of $0.97$ and $0.94$ on pretest, and $0.97$ and $0.92$ on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
